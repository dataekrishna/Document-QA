{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Document(PDF) Questioning & Answering - LlamaIndex, Llama2**\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "seamless interaction with PDF files, Chat with your PDF files using LlamaIndex, DataStax Astra DB (Apache Cassandra), and Gradient's open-source models, including LLama2\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "**Llama2**: Open source large language model\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**LlamaIndex**: Data framework for large language model applications\n",
        "\n",
        "---\n",
        "\n",
        "**Vector Database**: DataStax implementation of Apache Cassandra\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Gradient's LLM** is used to access the Llama2\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TmA_o1SnYnzy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "26OGhzT71u0n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1599af17-0d56-4699-da12-150098fc7144"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.8/18.8 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.4/270.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.1/225.1 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.0/284.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "#Installing the Libraries\n",
        "!pip install -q cassandra-driver\n",
        "!pip install -q cassio>=0.1.1\n",
        "!pip install -q gradientai --upgrade\n",
        "!pip install -q llama-index\n",
        "!pip install -q pypdf\n",
        "!pip install -q tiktoken==0.4.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the JSON Module and OS\n",
        "import os\n",
        "import json\n",
        "from google.colab import userdata\n",
        "#Inorder to use the Gradient LLM's we need the access tokens\n",
        "#Access Tokens and Workspace ID has to be retrived from Gradient workspace https://auth.gradient.ai/select-workspace\n",
        "os.environ['GRADIENT_ACCESS_TOKEN'] = userdata.get('GRADIENT_ACCESS_TOKEN')\n",
        "os.environ['GRADIENT_WORKSPACE_ID'] =  userdata.get('GRADIENT_WORKSPACE_ID')\n",
        "#The Tokens and work ID's has been stored in the Google colab Secrets(at the left side of the colab)"
      ],
      "metadata": {
        "id": "FA2sDj_J1yhs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing Llama index and Cassandra\n",
        "from llama_index import ServiceContext\n",
        "from llama_index import set_global_service_context\n",
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader, StorageContext\n",
        "from llama_index.embeddings import GradientEmbedding\n",
        "from llama_index.llms import GradientBaseModelLLM\n",
        "from llama_index.vector_stores import CassandraVectorStore\n",
        "import cassandra\n",
        "print (cassandra.__version__) #Checking the version of Cassandra has been imported"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BV7v3Ga03CPG",
        "outputId": "5ff3dca2-a3db-44ca-ca60-58b1d012bcbc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.29.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#To store the indexes we need Vector Database\n",
        "#Connecting to Vector Database - Datastax Astra DB\n",
        "#The below code is from DataStax Astra DB official Website. You can access to it after creating the Database in DataStax\n",
        "from cassandra.cluster import Cluster\n",
        "from cassandra.auth import PlainTextAuthProvider\n",
        "\n",
        "# This secure connect bundle is autogenerated when you download your SCB,\n",
        "# if yours is different update the file name below\n",
        "cloud_config= {\n",
        "  'secure_connect_bundle': 'secure-connect-fileqa-db.zip'  #Using Secure Connection Bundles when connecting to the database with drivers\n",
        "}      #This bundle can be downloaded, after creating the vector database while setting up the connection.\n",
        "\n",
        "# This token JSON file is autogenerated when you download your token,\n",
        "# if yours is different update the file name below\n",
        "with open(\"/content/fileqa_db-token.json\") as f:\n",
        "    secrets = json.load(f)\n",
        "\n",
        "CLIENT_ID = secrets[\"clientId\"]\n",
        "CLIENT_SECRET = secrets[\"secret\"]\n",
        "\n",
        "auth_provider = PlainTextAuthProvider(CLIENT_ID, CLIENT_SECRET)\n",
        "cluster = Cluster(cloud=cloud_config, auth_provider=auth_provider)\n",
        "session = cluster.connect()\n",
        "\n",
        "row = session.execute(\"select release_version from system.local\").one()\n",
        "if row:\n",
        "  print(row[0])\n",
        "else:\n",
        "  print(\"An error occurred.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrBdd0Wp3wtu",
        "outputId": "f8405b4e-f300-4410-a9f2-3d32fd7b288a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for aeadb65c-8c30-4665-ade6-686d29706503-us-east1.db.astra.datastax.com:29042:5099a42d-3208-46c7-a53a-3120eb62ff9e. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for aeadb65c-8c30-4665-ade6-686d29706503-us-east1.db.astra.datastax.com:29042:5099a42d-3208-46c7-a53a-3120eb62ff9e. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "ERROR:cassandra.connection:Closing connection <AsyncoreConnection(140025868700400) aeadb65c-8c30-4665-ade6-686d29706503-us-east1.db.astra.datastax.com:29042:5099a42d-3208-46c7-a53a-3120eb62ff9e> due to protocol error: Error from server: code=000a [Protocol error] message=\"Beta version of the protocol used (5/v5-beta), but USE_BETA flag is unset\"\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 5 to 4 for aeadb65c-8c30-4665-ade6-686d29706503-us-east1.db.astra.datastax.com:29042:5099a42d-3208-46c7-a53a-3120eb62ff9e. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.0.11-7807fa0fdf60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting up the Environment API Key\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "if not os.environ.get(\"GRADIENT_ACCESS_TOKEN\", None):\n",
        "    # Access token under https://auth.gradient.ai/select-workspace\n",
        "    os.environ[\"GRADIENT_ACCESS_TOKEN\"] = getpass(\"gradient.ai access token:\")\n",
        "if not os.environ.get(\"GRADIENT_WORKSPACE_ID\", None):\n",
        "    # `ID` listed in `$ gradient workspace list`\n",
        "    # also displayed after login at at https://auth.gradient.ai/select-workspace\n",
        "    os.environ[\"GRADIENT_WORKSPACE_ID\"] = getpass(\"gradient.ai workspace id:\")"
      ],
      "metadata": {
        "id": "yIqkxnk3Kjg6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the Gradient's Model Adapter for LLAMA-2\n",
        "\n",
        "from llama_index.llms import GradientBaseModelLLM\n",
        "\n",
        "# You can also use a model adapter you've trained with GradientModelAdapterLLM\n",
        "llm = GradientBaseModelLLM(\n",
        "    base_model_slug=\"llama2-7b-chat\",\n",
        "    max_tokens=400,\n",
        ")"
      ],
      "metadata": {
        "id": "RBR9lLyv4F11"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Configuring the Gradient embeddings\n",
        "#Embedding is used in order to generate the index\n",
        "embed_model = GradientEmbedding(             #Gradient Embedding function\n",
        "    gradient_access_token=os.environ[\"GRADIENT_ACCESS_TOKEN\"],\n",
        "    gradient_workspace_id=os.environ[\"GRADIENT_WORKSPACE_ID\"],\n",
        "    gradient_model_slug=\"bge-large\",\n",
        ")\n",
        "#Executing the LlamaIndex\n",
        "service_context = ServiceContext.from_defaults(\n",
        "    llm = llm,\n",
        "    embed_model = embed_model,\n",
        "    chunk_size=1024,\n",
        ")\n",
        "\n",
        "set_global_service_context(service_context)\n",
        "print(service_context)"
      ],
      "metadata": {
        "id": "wmPcBHgc4e6q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a9a8406-c9a7-4aea-8a56-88aa76f4c0a6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ServiceContext(llm_predictor=LLMPredictor(system_prompt=None, query_wrapper_prompt=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>), prompt_helper=PromptHelper(context_window=1024, num_output=400, chunk_overlap_ratio=0.1, chunk_size_limit=None, separator=' '), embed_model=GradientEmbedding(model_name='bge-large', embed_batch_size=10, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7f5a4d621660>), transformations=[SentenceSplitter(include_metadata=True, include_prev_next_rel=True, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7f5a4d621660>, id_func=<function default_id_func at 0x7f5a512405e0>, chunk_size=1024, chunk_overlap=200, separator=' ', paragraph_separator='\\n\\n\\n', secondary_chunking_regex='[^,.;。？！]+[,.;。？！]?')], llama_logger=<llama_index.logger.base.LlamaLogger object at 0x7f5a4f1003a0>, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7f5a4d621660>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader(\"/content/Documents\").load_data()\n",
        "print(f\"Loaded {len(documents)} document(s).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzcnKOQw4_3t",
        "outputId": "c093ea35-af1c-49b0-effe-4858f7a7f7f4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 101 document(s).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,service_context=service_context)\n",
        "query_engine = index.as_query_engine()"
      ],
      "metadata": {
        "id": "B1YSQnoH5aYO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Shoot the question you want!**"
      ],
      "metadata": {
        "id": "5z5AxCDYxirA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"What is the roadmap to learn SQL\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psACKQN76Zu_",
        "outputId": "09f00925-5fc0-4d6c-c8ce-46fe81f358e2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided context information, the roadmap to learn SQL can be broken down into the following steps:\n",
            "\n",
            "1. Learn the basics of SQL:\n",
            "\t* Understand the syntax and structure of SQL commands (SELECT, FROM, WHERE, GROUP BY, HAVING, etc.)\n",
            "\t* Learn how to write basic queries to retrieve data from a database\n",
            "\t* Familiarize yourself with the different types of SQL commands (e.g., SELECT, INSERT, UPDATE, DELETE)\n",
            "2. Learn how to use SQL to manipulate data:\n",
            "\t* Learn how to use SQL to insert, update, and delete data in a database\n",
            "\t* Understand how to use aggregate functions (e.g., SUM, AVG, COUNT) and subqueries\n",
            "\t* Learn how to use SQL to join multiple tables and retrieve data from related records\n",
            "3. Learn advanced SQL concepts:\n",
            "\t* Learn how to use SQL to perform complex queries, such as grouping and aggregating data\n",
            "\t* Understand how to use subqueries and common table expressions (CTEs)\n",
            "\t* Learn how to use SQL to perform transactions and handle errors\n",
            "4. Practice and gain hands-on experience:\n",
            "\t* Practice writing SQL queries using a database management system (DBMS) such as MySQL, SQL Server, or Oracle\n",
            "\t* Work on real-world projects to apply your SQL skills and gain practical experience\n",
            "5. Continuously learn and improve:\n",
            "\t* Keep up-to-date with the latest developments in SQL and database technology\n",
            "\t* Learn new features and capabilities of your chosen DBMS\n",
            "\t* Participate in online communities and forums to learn from others and share your knowledge\n",
            "\n",
            "By following this roadmap, you can gain a comprehensive understanding of SQL and become proficient in using it to manipulate and analyze data in a database\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CfluzYMcbUKX"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}