{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**PDF File Questioning & Answering - LlamaIndex, Llama2**\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "seamless interaction with PDF files, Chat with your PDF files using LlamaIndex, DataStax Astra DB (Apache Cassandra), and Gradient's open-source models, including LLama2\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "**Llama2**: Open source large language model\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**LlamaIndex**: Data framework for large language model applications\n",
        "\n",
        "---\n",
        "\n",
        "**Vector Database**: DataStax implementation of Apache Cassandra\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Gradient's LLM** ais used to access the Llama2\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TmA_o1SnYnzy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26OGhzT71u0n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad4c97a0-01aa-4f8e-e725-a05667e39e41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.7/169.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m929.3/929.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.0/143.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.5/221.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.9/277.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "#Installing the Libraries\n",
        "!pip install -q cassandra-driver\n",
        "!pip install -q cassio>=0.1.1\n",
        "!pip install -q gradientai --upgrade\n",
        "!pip install -q llama-index\n",
        "!pip install -q pypdf\n",
        "!pip install -q tiktoken==0.4.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the JSON Module and OS\n",
        "import os\n",
        "import json\n",
        "from google.colab import userdata\n",
        "#Inorder to use the Gradient LLM's we need the access tokens\n",
        "#Access Tokens and Workspace ID has to be retrived from Gradient workspace https://auth.gradient.ai/select-workspace\n",
        "os.environ['GRADIENT_ACCESS_TOKEN'] = userdata.get('GRADIENT_ACCESS_TOKEN')\n",
        "os.environ['GRADIENT_WORKSPACE_ID'] =  userdata.get('GRADIENT_WORKSPACE_ID')\n",
        "#The Tokens and work ID's has been stored in the Google colab Secrets(at the left side of the colab)"
      ],
      "metadata": {
        "id": "FA2sDj_J1yhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing Llama indeex and Cassandra\n",
        "from llama_index import ServiceContext\n",
        "from llama_index import set_global_service_context\n",
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader, StorageContext\n",
        "from llama_index.embeddings import GradientEmbedding\n",
        "from llama_index.llms import GradientBaseModelLLM\n",
        "from llama_index.vector_stores import CassandraVectorStore\n",
        "import cassandra\n",
        "print (cassandra.__version__) #Checking the version of Cassandra has been imported"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BV7v3Ga03CPG",
        "outputId": "6dc2dbba-67c7-45b7-b8b1-2bce6d6a1fa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#To store the indexes we need Vector Database\n",
        "#Connecting to Vector Database - Datastax Astra DB\n",
        "#The below code is from DataStax Astra DB official Website. You can access to it after creating the Database in DataStax\n",
        "from cassandra.cluster import Cluster\n",
        "from cassandra.auth import PlainTextAuthProvider\n",
        "\n",
        "# This secure connect bundle is autogenerated when you download your SCB,\n",
        "# if yours is different update the file name below\n",
        "cloud_config= {\n",
        "  'secure_connect_bundle': 'secure-connect-fileqa-db.zip'  #Using Secure Connection Bundles when connecting to the database with drivers\n",
        "}      #This bundle can be downloaded, after creating the vector database while setting up the connection.\n",
        "\n",
        "# This token JSON file is autogenerated when you download your token,\n",
        "# if yours is different update the file name below\n",
        "with open(\"/content/fileqa_db-token.json\") as f:\n",
        "    secrets = json.load(f)\n",
        "\n",
        "CLIENT_ID = secrets[\"clientId\"]\n",
        "CLIENT_SECRET = secrets[\"secret\"]\n",
        "\n",
        "auth_provider = PlainTextAuthProvider(CLIENT_ID, CLIENT_SECRET)\n",
        "cluster = Cluster(cloud=cloud_config, auth_provider=auth_provider)\n",
        "session = cluster.connect()\n",
        "\n",
        "row = session.execute(\"select release_version from system.local\").one()\n",
        "if row:\n",
        "  print(row[0])\n",
        "else:\n",
        "  print(\"An error occurred.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrBdd0Wp3wtu",
        "outputId": "282bc3cb-1d6b-46ed-de41-dd09d22bc145"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for aeadb65c-8c30-4665-ade6-686d29706503-us-east1.db.astra.datastax.com:29042:f88d0cb1-9d05-413f-bbb7-400aafa21446. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for aeadb65c-8c30-4665-ade6-686d29706503-us-east1.db.astra.datastax.com:29042:f88d0cb1-9d05-413f-bbb7-400aafa21446. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "ERROR:cassandra.connection:Closing connection <AsyncoreConnection(138560424797488) aeadb65c-8c30-4665-ade6-686d29706503-us-east1.db.astra.datastax.com:29042:f88d0cb1-9d05-413f-bbb7-400aafa21446> due to protocol error: Error from server: code=000a [Protocol error] message=\"Beta version of the protocol used (5/v5-beta), but USE_BETA flag is unset\"\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 5 to 4 for aeadb65c-8c30-4665-ade6-686d29706503-us-east1.db.astra.datastax.com:29042:f88d0cb1-9d05-413f-bbb7-400aafa21446. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.0.11-8a763e30acd8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting up the Environment API Key\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "if not os.environ.get(\"GRADIENT_ACCESS_TOKEN\", None):\n",
        "    # Access token under https://auth.gradient.ai/select-workspace\n",
        "    os.environ[\"GRADIENT_ACCESS_TOKEN\"] = getpass(\"gradient.ai access token:\")\n",
        "if not os.environ.get(\"GRADIENT_WORKSPACE_ID\", None):\n",
        "    # `ID` listed in `$ gradient workspace list`\n",
        "    # also displayed after login at at https://auth.gradient.ai/select-workspace\n",
        "    os.environ[\"GRADIENT_WORKSPACE_ID\"] = getpass(\"gradient.ai workspace id:\")"
      ],
      "metadata": {
        "id": "yIqkxnk3Kjg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the Gradient's Model Adapter for LLAMA-2\n",
        "\n",
        "from llama_index.llms import GradientBaseModelLLM\n",
        "\n",
        "# You can also use a model adapter you've trained with GradientModelAdapterLLM\n",
        "llm = GradientBaseModelLLM(\n",
        "    base_model_slug=\"llama2-7b-chat\",\n",
        "    max_tokens=400,\n",
        ")"
      ],
      "metadata": {
        "id": "RBR9lLyv4F11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Configuring the Gradient embeddings\n",
        "#Embedding is used in order to generate the index\n",
        "embed_model = GradientEmbedding(             #Gradient Embedding function\n",
        "    gradient_access_token=os.environ[\"GRADIENT_ACCESS_TOKEN\"],\n",
        "    gradient_workspace_id=os.environ[\"GRADIENT_WORKSPACE_ID\"],\n",
        "    gradient_model_slug=\"bge-large\",\n",
        ")\n",
        "#Executing the LlamaIndex\n",
        "service_context = ServiceContext.from_defaults(\n",
        "    llm = llm,\n",
        "    embed_model = embed_model,\n",
        "    chunk_size=1024,\n",
        ")\n",
        "\n",
        "set_global_service_context(service_context)\n",
        "print(service_context)"
      ],
      "metadata": {
        "id": "wmPcBHgc4e6q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b6478ab-6dd8-4069-cd44-51190c3949d6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ServiceContext(llm_predictor=LLMPredictor(system_prompt=None, query_wrapper_prompt=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>), prompt_helper=PromptHelper(context_window=1024, num_output=400, chunk_overlap_ratio=0.1, chunk_size_limit=None, separator=' '), embed_model=GradientEmbedding(model_name='bge-large', embed_batch_size=10, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7e051a2a7af0>), transformations=[SentenceSplitter(include_metadata=True, include_prev_next_rel=True, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7e051a2a7af0>, chunk_size=256, chunk_overlap=200, separator=' ', paragraph_separator='\\n\\n\\n', secondary_chunking_regex='[^,.;。？！]+[,.;。？！]?')], llama_logger=<llama_index.logger.base.LlamaLogger object at 0x7e051de070d0>, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7e051a2a7af0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader(\"/content/Documents\").load_data()\n",
        "print(f\"Loaded {len(documents)} document(s).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzcnKOQw4_3t",
        "outputId": "dbf71aa8-dffa-4a8f-f2bc-1e2536d9be25"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 101 document(s).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,service_context=service_context)\n",
        "query_engine = index.as_query_engine()"
      ],
      "metadata": {
        "id": "B1YSQnoH5aYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Shoot the question you wanted!**"
      ],
      "metadata": {
        "id": "5z5AxCDYxirA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"function of sql\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psACKQN76Zu_",
        "outputId": "9fd337c9-22ea-4433-cd65-4b0487cb229d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "There are several functions in SQL that can be used to perform calculations on data. Some of the most common functions include:\n",
            "\n",
            "1. AVG(): Returns the average value of a column.\n",
            "2. COUNT(): Returns the number of rows in a table.\n",
            "3. FIRST(): Returns the first value in a column.\n",
            "4. LAST(): Returns the last value in a column.\n",
            "5. MAX(): Returns the largest value in a column.\n",
            "6. MIN(): Returns the smallest value in a column.\n",
            "7. SUM(): Returns the sum of a numeric column.\n",
            "\n",
            "Additionally, there are scalar functions in SQL that can be used to perform calculations on a single value. Some of the most common scalar functions include:\n",
            "\n",
            "1. UCASE(): Converts a field to upper case.\n",
            "2. LCASE(): Converts a field to lower case.\n",
            "\n",
            "It is important to use aggregate functions and scalar functions appropriately in order to perform the desired calculations on data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CfluzYMcbUKX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}