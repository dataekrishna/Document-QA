# -*- coding: utf-8 -*-
"""Document_QA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iF1iFwv27odtaT3zHcK6p-1p3k3wqx_X

**Document(PDF) Questioning & Answering - LlamaIndex, Llama2**

---

---



seamless interaction with PDF files, Chat with your PDF files using LlamaIndex, DataStax Astra DB (Apache Cassandra), and Gradient's open-source models, including LLama2


---


---
**Llama2**: Open source large language model

---


**LlamaIndex**: Data framework for large language model applications

---

**Vector Database**: DataStax implementation of Apache Cassandra


---

**Gradient's LLM** is used to access the Llama2

---
"""

#Installing the Libraries
!pip install -q cassandra-driver
!pip install -q cassio>=0.1.1
!pip install -q gradientai --upgrade
!pip install -q llama-index
!pip install -q pypdf
!pip install -q tiktoken==0.4.0

#Importing the JSON Module and OS
import os
import json
from google.colab import userdata
#Inorder to use the Gradient LLM's we need the access tokens
#Access Tokens and Workspace ID has to be retrived from Gradient workspace https://auth.gradient.ai/select-workspace
os.environ['GRADIENT_ACCESS_TOKEN'] = userdata.get('GRADIENT_ACCESS_TOKEN')
os.environ['GRADIENT_WORKSPACE_ID'] =  userdata.get('GRADIENT_WORKSPACE_ID')
#The Tokens and work ID's has been stored in the Google colab Secrets(at the left side of the colab)

#Importing Llama index and Cassandra
from llama_index import ServiceContext
from llama_index import set_global_service_context
from llama_index import VectorStoreIndex, SimpleDirectoryReader, StorageContext
from llama_index.embeddings import GradientEmbedding
from llama_index.llms import GradientBaseModelLLM
from llama_index.vector_stores import CassandraVectorStore
import cassandra
print (cassandra.__version__) #Checking the version of Cassandra has been imported

#To store the indexes we need Vector Database
#Connecting to Vector Database - Datastax Astra DB
#The below code is from DataStax Astra DB official Website. You can access to it after creating the Database in DataStax
from cassandra.cluster import Cluster
from cassandra.auth import PlainTextAuthProvider

# This secure connect bundle is autogenerated when you download your SCB,
# if yours is different update the file name below
cloud_config= {
  'secure_connect_bundle': 'secure-connect-fileqa-db.zip'  #Using Secure Connection Bundles when connecting to the database with drivers
}      #This bundle can be downloaded, after creating the vector database while setting up the connection.

# This token JSON file is autogenerated when you download your token,
# if yours is different update the file name below
with open("/content/fileqa_db-token.json") as f:
    secrets = json.load(f)

CLIENT_ID = secrets["clientId"]
CLIENT_SECRET = secrets["secret"]

auth_provider = PlainTextAuthProvider(CLIENT_ID, CLIENT_SECRET)
cluster = Cluster(cloud=cloud_config, auth_provider=auth_provider)
session = cluster.connect()

row = session.execute("select release_version from system.local").one()
if row:
  print(row[0])
else:
  print("An error occurred.")

#Setting up the Environment API Key
import os
from getpass import getpass

if not os.environ.get("GRADIENT_ACCESS_TOKEN", None):
    # Access token under https://auth.gradient.ai/select-workspace
    os.environ["GRADIENT_ACCESS_TOKEN"] = getpass("gradient.ai access token:")
if not os.environ.get("GRADIENT_WORKSPACE_ID", None):
    # `ID` listed in `$ gradient workspace list`
    # also displayed after login at at https://auth.gradient.ai/select-workspace
    os.environ["GRADIENT_WORKSPACE_ID"] = getpass("gradient.ai workspace id:")

#Define the Gradient's Model Adapter for LLAMA-2

from llama_index.llms import GradientBaseModelLLM

# You can also use a model adapter you've trained with GradientModelAdapterLLM
llm = GradientBaseModelLLM(
    base_model_slug="llama2-7b-chat",
    max_tokens=400,
)

#Configuring the Gradient embeddings
#Embedding is used in order to generate the index
embed_model = GradientEmbedding(             #Gradient Embedding function
    gradient_access_token=os.environ["GRADIENT_ACCESS_TOKEN"],
    gradient_workspace_id=os.environ["GRADIENT_WORKSPACE_ID"],
    gradient_model_slug="bge-large",
)
#Executing the LlamaIndex
service_context = ServiceContext.from_defaults(
    llm = llm,
    embed_model = embed_model,
    chunk_size=1024,
)

set_global_service_context(service_context)
print(service_context)

documents = SimpleDirectoryReader("/content/Documents").load_data()
print(f"Loaded {len(documents)} document(s).")

index = VectorStoreIndex.from_documents(
    documents,service_context=service_context)
query_engine = index.as_query_engine()

"""# **Shoot the question you want!**"""

response = query_engine.query("What is the roadmap to learn SQL")
print(response)

